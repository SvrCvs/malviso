{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqwpPeHuURHD"
      },
      "source": [
        "Scopo degli algoritmi di Computer Vision da usare nel nostro progetto:\n",
        "\n",
        "\n",
        "    Input: Immagine \n",
        "\n",
        "         --> Detector [Immagine] = {Array di keypoints} \n",
        "\n",
        "         --> Descriptor [{Array di keypoints}] = {Matrice di features}\n",
        "\n",
        "(Ogni Keypoint viene descritto da un numero/tipo di features differenti che dipende dall'algoritmo usato. SIFT per esempio sta per \"Scale Invariant Feature Transform\" e si base su features preservabili da trasformazioni spaziali ([roto-traslazioni / restizioni-espansioni])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_hHei5VjJnQ1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the path to the two folders\n",
        "folder_1_path = \"dataset/byteplot/malicious/\"\n",
        "folder_2_path = \"dataset/byteplot/benign/benign/\"\n",
        "folder_3_path = \"dataset/byteplot/benign/benign_edu/\"\n",
        "\n",
        "# Read the images in each folder and store them in a list\n",
        "images_1 = [cv2.imread(os.path.join(folder_1_path, image_file), cv2.IMREAD_GRAYSCALE) \\\n",
        "            for image_file in os.listdir(folder_1_path)]\n",
        "images_2 = [cv2.imread(os.path.join(folder_2_path, image_file), cv2.IMREAD_GRAYSCALE) \\\n",
        "            for image_file in os.listdir(folder_2_path)]\n",
        "images_3 = [cv2.imread(os.path.join(folder_3_path, image_file), cv2.IMREAD_GRAYSCALE) \\\n",
        "            for image_file in os.listdir(folder_3_path)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "random.shuffle(images_1)\n",
        "\n",
        "max_len_benign = len(images_2)+len(images_3)\n",
        "\n",
        "images_1 = images_1[:max_len_benign]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label each image with the respective label (1 for total_des_trainfolder_1, 2 for folder_2)\n",
        "labels_1 = [0 for _ in range(len(images_1))]\n",
        "labels_2 = [1 for _ in range(len(images_2)+len(images_3))]\n",
        "\n",
        "# Combine the images and labels into a single dataset\n",
        "images = images_1 + images_2 + images_3\n",
        "labels = labels_1 + labels_2\n",
        "\n",
        "X = images\n",
        "y = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5936\n",
            "5936\n"
          ]
        }
      ],
      "source": [
        "print(len(X))\n",
        "print(len(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDjQzK9xteHt"
      },
      "source": [
        "Implementazione SIFT/ORB descriptors. Partendo dal file path, generiamo di nuovo il bytre plot, ma questa volta evidenziamo i keypoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define detector object\n",
        "nfeatures = 41\n",
        "detector = cv2.ORB_create(nfeatures)\n",
        "\n",
        "a=[]\n",
        "b=[]\n",
        "c=[]\n",
        "d=[]\n",
        "\n",
        "for image in X_train:\n",
        "    query_kps, query_des = detector.detectAndCompute(image, None)\n",
        "    a.append(query_kps)\n",
        "    b.append(query_des)\n",
        "\n",
        "for image in X_test:\n",
        "    query_kps, query_des = detector.detectAndCompute(image, None)\n",
        "    c.append(query_kps)\n",
        "    d.append(query_des)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_kps_train= a\n",
        "total_des_train= b\n",
        "total_kps_test= c\n",
        "total_des_test= d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "null_indexes_train = []\n",
        "\n",
        "for i,e in enumerate(total_des_train):\n",
        "    if e is None:\n",
        "        null_indexes_train.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_des_train = np.delete(total_des_train, null_indexes_train) \n",
        "y_train = np.delete(y_train,null_indexes_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_des_train = total_des_train.tolist()\n",
        "y_train = y_train.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "null_indexes_test = []\n",
        "\n",
        "for i,e in enumerate(total_des_test):\n",
        "    if e is None:\n",
        "        null_indexes_test.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_des_test = np.delete(total_des_test, null_indexes_test) \n",
        "y_test = np.delete(y_test,null_indexes_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_des_test = total_des_test.tolist()\n",
        "y_test = y_test.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "max0 = 0\n",
        "for des in total_des_train:\n",
        "    if des is not None:\n",
        "        if des.shape[0] > max0:\n",
        "            max0 = des.shape[0]\n",
        "\n",
        "max1=0\n",
        "for des in total_des_test:\n",
        "    if des is not None:\n",
        "        if des.shape[0] > max1:\n",
        "            max1 = des.shape[0]\n",
        "    \n",
        "if max1 > max0:\n",
        "    max0=max1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "for des in total_des_train:\n",
        "    padding_to_add = max0 - des.shape[0]\n",
        "    if padding_to_add != 0:\n",
        "        pad_list = []\n",
        "        for k in range(32):\n",
        "            pad_list.append(0)\n",
        "        for j in range(padding_to_add):\n",
        "            total_des_train[i] = np.append(total_des_train[i], pad_list)\n",
        "        total_des_train[i] = np.array(total_des_train[i]).reshape(max0,32)\n",
        "    i += 1\n",
        "\n",
        "i = 0\n",
        "for des in total_des_test:\n",
        "    padding_to_add = max0 - des.shape[0]\n",
        "    if padding_to_add != 0:\n",
        "        pad_list = []\n",
        "        for k in range(32):\n",
        "            pad_list.append(0)\n",
        "        for j in range(padding_to_add):\n",
        "            total_des_test[i] = np.append(total_des_test[i], pad_list)\n",
        "        total_des_test[i] = np.array(total_des_test[i]).reshape(max0,32)\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "for des in total_des_train:\n",
        "    total_des_train[i] = np.hstack(des)\n",
        "    i += 1\n",
        "\n",
        "i = 0\n",
        "for des in total_des_test:\n",
        "    total_des_test[i] = np.hstack(des)\n",
        "    i += 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "gridsearch (using accuracy as a metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the parameters to search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 4, 8],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create an instance of the RandomForestClassifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create an instance of GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(total_des_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Get the best classifier\n",
        "best_clf = grid_search.best_estimator_\n",
        "\n",
        "# Use cross_val_score to evaluate the classifier with the best parameters\n",
        "scores = cross_val_score(best_clf, total_des_test, y_test, cv=10)\n",
        "\n",
        "# Print the mean and standard deviation of the scores\n",
        "print(\"Mean accuracy: \", scores.mean())\n",
        "print(\"Standard deviation: \", scores.std())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "gridsearch (using F1 as a metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the parameters to search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 4, 8],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create an instance of the RandomForestClassifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create an instance of GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(total_des_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Get the best classifier\n",
        "best_clf = grid_search.best_estimator_\n",
        "\n",
        "# Use cross_val_score to evaluate the classifier with the best parameters\n",
        "scores = cross_val_score(best_clf, total_des_test, y_test, cv=10)\n",
        "\n",
        "# Print the mean and standard deviation of the scores\n",
        "print(\"Mean accuracy: \", scores.mean())\n",
        "print(\"Standard deviation: \", scores.std())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The array in input to fit needs to be 2d array/list, on the x axis you have the number of samples on the y axis you have the n of elements for that samples. (e.g. on an array with 1000 images and for every images a (32,32) matrix of descriptors, you would need to stack the matrix vertically having a 1d array of 1024 elements. Then,array to give in input to fit would be (1000,1024) and would be accepted by fit).\n",
        "\n",
        "All the elements in the input array have to be of the same size. For example, on the previous array we can't have 999 samples having 1024 points and 1 sample having 1023 points. The fit method in this case will throw you an error similar to \"the homogenous size is not correct found(x+,) etc...\".\n",
        "\n",
        "Trivially, the number of samples and the number of labels have to match.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7363100252737995\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "clf.fit(total_des_train, y_train)\n",
        "\n",
        "score = clf.score(total_des_test, y_test)\n",
        "print(\"Test accuracy:\", score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_values(nfeatures, X_train, X_test, y_train, y_test):\n",
        "    detector = cv2.ORB_create(nfeatures)\n",
        "\n",
        "    total_kps_train= []\n",
        "    total_des_train= []\n",
        "    total_kps_test= []\n",
        "    total_des_test= []\n",
        "\n",
        "    for image in X_train:\n",
        "        query_kps, query_des = detector.detectAndCompute(image, None)\n",
        "        total_kps_train.append(query_kps)\n",
        "        total_des_train.append(query_des)\n",
        "\n",
        "    for image in X_test:\n",
        "        query_kps, query_des = detector.detectAndCompute(image, None)\n",
        "        total_kps_test.append(query_kps)\n",
        "        total_des_test.append(query_des)\n",
        "\n",
        "\n",
        "    null_indexes_train = []\n",
        "\n",
        "    for i,e in enumerate(total_des_train):\n",
        "        if e is None:\n",
        "            null_indexes_train.append(i)\n",
        "\n",
        "    total_des_train = np.delete(total_des_train, null_indexes_train).tolist()\n",
        "    y_train = np.delete(y_train,null_indexes_train).tolist()\n",
        "\n",
        "\n",
        "\n",
        "    null_indexes_test = []\n",
        "\n",
        "    for i,e in enumerate(total_des_test):\n",
        "        if e is None:\n",
        "            null_indexes_test.append(i)\n",
        "\n",
        "    total_des_test = np.delete(total_des_test, null_indexes_test).tolist() \n",
        "    y_test = np.delete(y_test,null_indexes_test).tolist()\n",
        "\n",
        "\n",
        "\n",
        "    max0 = 0\n",
        "    for des in total_des_train:\n",
        "        if des is not None:\n",
        "            if des.shape[0] > max0:\n",
        "                max0 = des.shape[0]\n",
        "\n",
        "    max1=0\n",
        "    for des in total_des_test:\n",
        "        if des is not None:\n",
        "            if des.shape[0] > max1:\n",
        "                max1 = des.shape[0]\n",
        "        \n",
        "    if max1 > max0:\n",
        "        max0=max1\n",
        "\n",
        "\n",
        "\n",
        "    i = 0\n",
        "    for des in total_des_train:\n",
        "        padding_to_add = max0 - des.shape[0]\n",
        "        if padding_to_add != 0:\n",
        "            pad_list = []\n",
        "            for k in range(32):\n",
        "                pad_list.append(0)\n",
        "            for j in range(padding_to_add):\n",
        "                total_des_train[i] = np.append(total_des_train[i], pad_list)\n",
        "            total_des_train[i] = np.array(total_des_train[i]).reshape(max0,32)\n",
        "        i += 1\n",
        "\n",
        "    i = 0\n",
        "    for des in total_des_test:\n",
        "        padding_to_add = max0 - des.shape[0]\n",
        "        if padding_to_add != 0:\n",
        "            pad_list = []\n",
        "            for k in range(32):\n",
        "                pad_list.append(0)\n",
        "            for j in range(padding_to_add):\n",
        "                total_des_test[i] = np.append(total_des_test[i], pad_list)\n",
        "            total_des_test[i] = np.array(total_des_test[i]).reshape(max0,32)\n",
        "        i += 1\n",
        "\n",
        "\n",
        "    i = 0\n",
        "    for des in total_des_train:\n",
        "        total_des_train[i] = np.hstack(des)\n",
        "        i += 1\n",
        "\n",
        "    i = 0\n",
        "    for des in total_des_test:\n",
        "        total_des_test[i] = np.hstack(des)\n",
        "        i += 1\n",
        "\n",
        "    return total_des_train, total_des_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20  Test accuracy:  0.7464195450716091\n",
            "21  Test accuracy:  0.7489469250210615\n",
            "22  Test accuracy:  0.7379949452401011\n",
            "23  Test accuracy:  0.7388374052232519\n",
            "24  Test accuracy:  0.7346251053074979\n",
            "25  Test accuracy:  0.7489469250210615\n",
            "26  Test accuracy:  0.7379949452401011\n",
            "27  Test accuracy:  0.7447346251053075\n",
            "28  Test accuracy:  0.7413647851727043\n",
            "29  Test accuracy:  0.7438921651221567\n",
            "30  Test accuracy:  0.7388374052232519\n",
            "31  Test accuracy:  0.7177759056444819\n",
            "32  Test accuracy:  0.7363100252737995\n",
            "33  Test accuracy:  0.7430497051390059\n",
            "34  Test accuracy:  0.7346251053074979\n",
            "35  Test accuracy:  0.7481044650379107\n",
            "36  Test accuracy:  0.7388374052232519\n",
            "37  Test accuracy:  0.7497893850042123\n",
            "38  Test accuracy:  0.7245155855096883\n",
            "39  Test accuracy:  0.7337826453243471\n",
            "40  Test accuracy:  0.7464195450716091\n",
            "41  Test accuracy:  0.7582139848357203\n",
            "42  Test accuracy:  0.7295703454085931\n",
            "43  Test accuracy:  0.7464195450716091\n",
            "44  Test accuracy:  0.7228306655433867\n",
            "45  Test accuracy:  0.7253580454928391\n",
            "46  Test accuracy:  0.7160909856781803\n",
            "47  Test accuracy:  0.7270429654591407\n",
            "48  Test accuracy:  0.7101937657961247\n",
            "49  Test accuracy:  0.7135636057287279\n"
          ]
        }
      ],
      "source": [
        "for par in range(20,50):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    total_des_train, total_des_test, y_train, y_test = get_values(par, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "    clf.fit(total_des_train, y_train)\n",
        "\n",
        "    score = clf.score(total_des_test, y_test)\n",
        "    print(par, \" Test accuracy: \", score)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best test accuracy with 41 features. Very similar to the actual number of features described in the dataset's paper! \n",
        "Outperforms the baseline made by 32 features (chosen to replicate \"Robust malware detection\" paper)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
