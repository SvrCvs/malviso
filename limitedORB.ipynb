{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqwpPeHuURHD"
      },
      "source": [
        "Scopo degli algoritmi di Computer Vision da usare nel nostro progetto:\n",
        "\n",
        "\n",
        "    Input: Immagine \n",
        "\n",
        "         --> Detector [Immagine] = {Array di keypoints} \n",
        "\n",
        "         --> Descriptor [{Array di keypoints}] = {Matrice di features}\n",
        "\n",
        "(Ogni Keypoint viene descritto da un numero/tipo di features differenti che dipende dall'algoritmo usato. SIFT per esempio sta per \"Scale Invariant Feature Transform\" e si base su features preservabili da trasformazioni spaziali ([roto-traslazioni / restizioni-espansioni])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_hHei5VjJnQ1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the path to the two folders\n",
        "folder_1_path = \"dataset/byteplot/malicious/\"\n",
        "folder_2_path = \"dataset/byteplot/benign/benign/\"\n",
        "folder_3_path = \"dataset/byteplot/benign/benign_edu/\"\n",
        "\n",
        "# Read the images in each folder and store them in a list\n",
        "images_1 = [cv2.imread(os.path.join(folder_1_path, image_file), cv2.IMREAD_GRAYSCALE) \\\n",
        "            for image_file in os.listdir(folder_1_path)]\n",
        "images_2 = [cv2.imread(os.path.join(folder_2_path, image_file), cv2.IMREAD_GRAYSCALE) \\\n",
        "            for image_file in os.listdir(folder_2_path)]\n",
        "images_3 = [cv2.imread(os.path.join(folder_3_path, image_file), cv2.IMREAD_GRAYSCALE) \\\n",
        "            for image_file in os.listdir(folder_3_path)]\n",
        "\n",
        "# Label each image with the respective label (1 for total_des_trainfolder_1, 2 for folder_2)\n",
        "labels_1 = [0 for _ in range(len(images_1))]\n",
        "#labels_2 = [1 for _ in range(len(images_2))]\n",
        "labels_2 = [1 for _ in range(len(images_2)+len(images_3))]\n",
        "\n",
        "# Combine the images and labels into a single dataset\n",
        "images = images_1 + images_2 + images_3\n",
        "labels = labels_1 + labels_2\n",
        "\n",
        "X = images\n",
        "y = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDjQzK9xteHt"
      },
      "source": [
        "Implementazione SIFT/ORB descriptors. Partendo dal file path, generiamo di nuovo il bytre plot, ma questa volta evidenziamo i keypoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define detector object\n",
        "detector = cv2.ORB_create(nfeatures = 32)\n",
        "\n",
        "total_kps_train=[]\n",
        "total_des_train=[]\n",
        "total_kps_test=[]\n",
        "total_des_test=[]\n",
        "\n",
        "for image in X_train:\n",
        "    query_kps, query_des = detector.detectAndCompute(image, None)\n",
        "    total_kps_train.append(query_kps)\n",
        "    total_des_train.append(query_des)\n",
        "\n",
        "for image in X_test:\n",
        "    query_kps, query_des = detector.detectAndCompute(image, None)\n",
        "    total_kps_test.append(query_kps)\n",
        "    total_des_test.append(query_des)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "max = 0\n",
        "i=0\n",
        "for des in total_des_train:\n",
        "    if des is not None:\n",
        "        #print(i)\n",
        "        #print(des.shape[0])\n",
        "        if des.shape[0] > max:\n",
        "            max = des.shape[0]\n",
        "    else:\n",
        "        del total_des_train[i]\n",
        "        del y_train[i]\n",
        "    i+= 1\n",
        "\n",
        "max1=0\n",
        "i=0\n",
        "for des in total_des_test:\n",
        "    if des is not None:\n",
        "        #print(i)\n",
        "        #print(des.shape[0])\n",
        "        if des.shape[0] > max1:\n",
        "            max1 = des.shape[0]\n",
        "    else:\n",
        "        del total_des_test[i]\n",
        "        del y_test[i]\n",
        "    i+= 1\n",
        "    \n",
        "if max1 > max:\n",
        "    max=max1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_des_train_ex = []\n",
        "y_train_ex = []\n",
        "total_des_test_ex = []\n",
        "y_test_ex = []\n",
        "\n",
        "i = 0\n",
        "for des in total_des_train:\n",
        "    padding_to_add = max - des.shape[0]\n",
        "    if padding_to_add != 0:\n",
        "        pad_list = []\n",
        "        for k in range(32):\n",
        "            pad_list.append(0)\n",
        "        for j in range(padding_to_add):\n",
        "            total_des_train[i] = np.append(total_des_train[i], pad_list)\n",
        "        total_des_train[i] = np.array(total_des_train[i]).reshape(max,32)\n",
        "    i += 1\n",
        "\n",
        "i = 0\n",
        "for des in total_des_test:\n",
        "    padding_to_add = max - des.shape[0]\n",
        "    if padding_to_add != 0:\n",
        "        pad_list = []\n",
        "        for k in range(32):\n",
        "            pad_list.append(0)\n",
        "        for j in range(padding_to_add):\n",
        "            total_des_test[i] = np.append(total_des_test[i], pad_list)\n",
        "        total_des_test[i] = np.array(total_des_test[i]).reshape(max,32)\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "for des in total_des_train:\n",
        "    total_des_train[i] = np.hstack(des)\n",
        "    i += 1\n",
        "\n",
        "i = 0\n",
        "for des in total_des_test:\n",
        "    total_des_test[i] = np.hstack(des)\n",
        "    i += 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The array in input to fit needs to be 2d array/list, on the x axis you have the number of samples on the y axis you have the n of elements for that samples. (e.g. on an array with 1000 images and for every images a (32,32) matrix of descriptors, you would need to stack the matrix vertically having a 1d array of 1024 elements. Then,array to give in input to fit would be (1000,1024) and would be accepted by fit).\n",
        "\n",
        "All the elements in the input array have to be of the same size. For example, on the previous array we can't have 999 samples having 1024 points and 1 sample having 1023 points. The fit method in this case will throw you an error similar to \"the homogenous size is not correct found(x+,) etc...\".\n",
        "\n",
        "Trivially, the number of samples and the number of labels have to match.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.8570658036677454\n"
          ]
        }
      ],
      "source": [
        "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "clf.fit(total_des_train, y_train)\n",
        "\n",
        "score = clf.score(total_des_test, y_test)\n",
        "print(\"Test accuracy:\", score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
